Replay Memory:

  Everytime that the agent performs an action, either by exploring or exploiting, the agent lives an experience. For the purpose of training the algorithm, we will store all these experiences.

  Experiences are formed by the initial state, the action taken, the state reached (final state) and the reward gotten and they are stored in the Replay Memory. Then, every time that an action is taken, the algorithm is trained following this steps:

  \begin{enumerate}
    \item Replay Memory checks if the number of experiences is higher than the batch size
    \item If there are enough experiences:
    \begin{enumerate}
      \item Replay Memory supplies a random set of experiences of size=batch_size.
      \item With this set of experiences, the target network is trained.
    \end{enumerate}
  \end{enumerate}

  Optimizing Replay Memory can be a challenge, because, if we are using a Graphic Card in the training, we would be storing all the experiences in its memory. But, why do we need to store all the experiences? We could also be using the last N experiences to train the network and it would be a less memory-consumption demanding solution. The answer to this question is tha
  
