
@online{noauthor_formacion_nodate,
	title = {Formación en línea de {CB}3},
	url = {https://academy.universal-robots.com/es/formacion-en-linea-gratuita/formacion-en-linea-de-cb3/},
	urldate = {2020-11-15},
}

@online{noauthor_python_nodate,
	title = {python - Saving an Object (Data persistence)},
	url = {https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence},
	titleaddon = {Stack Overflow},
	urldate = {2020-11-15},
}

@online{noauthor_pytorch_nodate,
	title = {{PyTorch} on the {GPU} - Training Neural Networks with {CUDA}},
	url = {https://deeplizard.com/learn/video/Bs1mdHZiAS8},
	abstract = {Welcome to this neural network programming series! In this episode, we will see how we can use the {CUDA} capabilities of {PyTorch} to run our code on the {GPU}.},
	urldate = {2020-11-15},
	langid = {english},
}

@video{preferred_networks_inc_bin-picking_2015,
	title = {Bin-picking Robot Deep Learning},
	url = {https://www.youtube.com/watch?v=ydh_AdWZflA&ab_channel=Pickit3D},
	abstract = {Autonomous learning of bin-picking robot.
	Achieved human level performance within 8 hours},
	author = {{Preferred Networks, Inc.}},
	urldate = {2020-11-27},
	date = {2015},
}

@online{noauthor_reinforcement_nodate,
	title = {Reinforcement Learning - Goal Oriented Intelligence},
	url = {https://deeplizard.com/learn/playlist/PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv},
	abstract = {This series is all about reinforcement learning ({RL})! Here, we’ll gain an understanding of the intuition, the math, and the coding involved with {RL}. We’ll first start out with an introduction to {RL} wh},
	urldate = {2020-11-28},
	langid = {english},
	file = {Snapshot:C\:\\Users\\pablo\\Zotero\\storage\\DSY2MV9W\\PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv.html:text/html},
}


@online{openai_gym_nodate,
	title = {Gym: A toolkit for developing and comparing reinforcement learning algorithms},
	url = {https://gym.openai.com},
	shorttitle = {Gym},
	abstract = {A toolkit for developing and comparing reinforcement learning algorithms},
	author = {{OpenAI}},
	urldate = {2020-11-30},
	file = {Snapshot:C\:\\Users\\pablo\\Zotero\\storage\\WHBH2G8A\\gym.openai.com.html:text/html},
}


@article{mahmood_setting_2018,
	title = {Setting up a Reinforcement Learning Task with a Real-World Robot},
	url = {http://arxiv.org/abs/1803.07067},
	abstract = {Reinforcement learning is a promising approach to developing hard-to-engineer adaptive solutions for complex and diverse robotic tasks. However, learning with real-world robots is often unreliable and difficult, which resulted in their low adoption in reinforcement learning research. This difficulty is worsened by the lack of guidelines for setting up learning tasks with robots. In this work, we develop a learning task with a {UR}5 robotic arm to bring to light some key elements of a task setup and study their contributions to the challenges with robots. We find that learning performance can be highly sensitive to the setup, and thus oversights and omissions in setup details can make effective learning, reproducibility, and fair comparison hard. Our study suggests some mitigating steps to help future experimenters avoid difficulties and pitfalls. We show that highly reliable and repeatable experiments can be performed in our setup, indicating the possibility of reinforcement learning research extensively based on real-world robots.},
	journaltitle = {{arXiv}:1803.07067 [cs, stat]},
	author = {Mahmood, A. Rupam and Korenkevych, Dmytro and Komer, Brent J. and Bergstra, James},
	urldate = {2020-11-30},
	date = {2018-03-19},
	eprinttype = {arxiv},
	eprint = {1803.07067},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}


@article{lample_playing_2018,
	title = {Playing {FPS} Games with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1609.05521},
	abstract = {Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in {AI} agents of the game as well as humans in deathmatch scenarios.},
	journaltitle = {{arXiv}:1609.05521 [cs]},
	author = {Lample, Guillaume and Chaplot, Devendra Singh},
	urldate = {2020-11-30},
	date = {2018-01-29},
	eprinttype = {arxiv},
	eprint = {1609.05521},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pablo\\Zotero\\storage\\9HFZWPIG\\Lample y Chaplot - 2018 - Playing FPS Games with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pablo\\Zotero\\storage\\S6Y4FSF6\\1609.html:text/html},
}


@inproceedings{zhu_target-driven_2017,
	title = {Target-driven visual navigation in indoor scenes using deep reinforcement learning},
	doi = {10.1109/ICRA.2017.7989381},
	abstract = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the {AI}2-{THOR} framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.},
	eventtitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {3357--3364},
	booktitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	author = {Zhu, Y. and Mottaghi, R. and Kolve, E. and Lim, J. J. and Gupta, A. and Fei-Fei, L. and Farhadi, A.},
	date = {2017-05},
	keywords = {actor-critic model, {AI}2-{THOR} framework, deep reinforcement learning, high-quality 3D scenes, indoor scenes, learning (artificial intelligence), Learning (artificial intelligence), Navigation, path planning, Physics, physics engine, real robot scenario, robot vision, Robots, target-driven visual navigation, Three-dimensional displays, Training, Visualization},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\pablo\\Zotero\\storage\\KU7W7IBK\\7989381.html:text/html;Versión enviada:C\:\\Users\\pablo\\Zotero\\storage\\62IF7Y4A\\Zhu et al. - 2017 - Target-driven visual navigation in indoor scenes u.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\pablo\\Zotero\\storage\\9UXXHAH7\\7989381.html:text/html},
}


@article{lei_dynamic_2018,
	title = {Dynamic Path Planning of Unknown Environment Based on Deep Reinforcement Learning},
	volume = {2018},
	issn = {1687-9600, 1687-9619},
	url = {https://www.hindawi.com/journals/jr/2018/5781591/},
	doi = {10.1155/2018/5781591},
	abstract = {Dynamic path planning of unknown environment has always been a challenge for mobile robots. In this paper, we apply double Q-network ({DDQN}) deep reinforcement learning proposed by {DeepMind} in 2016 to dynamic path planning of unknown environment. The reward and punishment function and the training method are designed for the instability of the training stage and the sparsity of the environment state space. In different training stages, we dynamically adjust the starting position and target position. With the updating of neural network and the increase of greedy rule probability, the local space searched by agent is expanded. Pygame module in {PYTHON} is used to establish dynamic environments. Considering lidar signal and local target position as the inputs, convolutional neural networks ({CNNs}) are used to generalize the environmental state. Q-learning algorithm enhances the ability of the dynamic obstacle avoidance and local planning of the agents in environment. The results show that, after training in different dynamic environments and testing in a new environment, the agent is able to reach the local target position successfully in unknown dynamic environment.},
	pages = {1--10},
	journaltitle = {Journal of Robotics},
	shortjournal = {Journal of Robotics},
	author = {Lei, Xiaoyun and Zhang, Zhian and Dong, Peifang},
	urldate = {2020-11-30},
	date = {2018-09-18},
	langid = {english},
	file = {Lei et al. - 2018 - Dynamic Path Planning of Unknown Environment Based.pdf:C\:\\Users\\pablo\\Zotero\\storage\\XWEJHI89\\Lei et al. - 2018 - Dynamic Path Planning of Unknown Environment Based.pdf:application/pdf},
}


@article{wiering_reinforcement_2001,
	title = {Reinforcement Learning in Dynamic Environments using Instantiated Information},
	abstract = {We study using reinforcement learning in dynamic environments. Such environments may contain many dynamic objects which makes optimal planning hard. One way of using information about all dynamic objects is to expand the state description, but this results in a high dimensional policy space. Our approach is to instantiate information about dynamic objects in the model of the environment and to replan using model-based reinforcement learning whenever this information changes. Furthermore, our approach is combined with an a-priori model of the changing parts of the environment, which enables the agent to optimally plan a course of action. Results on a navigation task with multiple dynamic hostile agents show that our system is able to learn good solutions minimizing the risk of hitting hostile agents.},
	author = {Wiering, Marco},
	date = {2001-08-27},
}


@online{neshovski_sustainable_nodate,
	title = {Sustainable Development Goals},
	url = {https://www.un.org/sustainabledevelopment/},
	abstract = {For the latest United Nations information on the coronavirus, please visit www.un.org/coronavirus.     
	17 Goals to Transform Our World
	The Sustainable Development Goals are a call for action by all countries - poor, rich and middle-income},
	titleaddon = {United Nations Sustainable Development},
	author = {Neshovski, Robert},
	urldate = {2020-11-30},
	langid = {american},
}


@online{noauthor_agile_nodate,
	title = {Agile Methodology: What is Agile Software Development Model?},
	url = {https://www.guru99.com/agile-scrum-extreme-testing.html},
	urldate = {2020-12-02},
	file = {Agile Methodology\: What is Agile Software Development Model?:C\:\\Users\\pablo\\Zotero\\storage\\RY6WUWCL\\agile-scrum-extreme-testing.html:text/html},
}